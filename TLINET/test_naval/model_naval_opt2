import torch
import torch.nn.functional as F
from torch import nn
import pickle, os, random as rd
from utils import *
from neurons import *

def soft_min(x, beta=10.0):
    return -1.0 / beta * torch.logsumexp(-beta * x, dim=1)

def model_naval(train_data, train_label, val_data, val_label, n_iters, round, avm, variable_based):
    nsample = train_data.shape[0]
    val_nsample = val_data.shape[0]
    length = train_data.shape[1]
    dim = train_data.shape[2]

    nf, nc = 8, 2
    t1 = torch.randint(0, length-1, (nf,), dtype=torch.float64, requires_grad=True)
    t2 = torch.randint(0, length-1, (nf,), dtype=torch.float64, requires_grad=True)
    a = torch.rand(nf, dtype=torch.float64, requires_grad=True)
    b = torch.rand(nf, dtype=torch.float64, requires_grad=True)
    pc = torch.rand((nc, nf), requires_grad=True)
    pd = torch.rand((1, nc), requires_grad=True)
    tvar_temporal = torch.rand(nf, dtype=torch.float64, requires_grad=True)
    tvar_logical_c = torch.rand(nc, dtype=torch.float64, requires_grad=True)
    tvar_logical_d = torch.rand(1, dtype=torch.float64, requires_grad=True)
    dimension = [0, 1] * (nf // 2)
    temporal_type = ['temporal'] * nf if variable_based else ['F'] * (nf // 2) + ['G'] * (nf // 2)
    logical_in = 'logical' if variable_based else 'and'
    logical_out = 'logical' if variable_based else 'or'

    tau_pos = torch.tensor(0.1, dtype=torch.float64, requires_grad=True)
    tau_neg = torch.tensor(0.1, dtype=torch.float64, requires_grad=True)

    file = f'/initialization/case{round}.pkl'
    path = os.getcwd() + file
    if os.path.isfile(path):
        with open(path, 'rb') as f:
            a, b, t1, t2, pc, pd, tvar_temporal, tvar_logical_c, tvar_logical_d = pickle.load(f)
    else:
        os.makedirs(os.path.join(os.getcwd(), 'initialization'), exist_ok=True)
        with open(os.path.join(os.getcwd(), 'initialization', f'case{round}.pkl'), 'wb') as f:
            pickle.dump([a, b, t1, t2, pc, pd, tvar_temporal, tvar_logical_c, tvar_logical_d], f)

    pred = [Predicate(a[i], b[i], dim=dimension[i]) for i in range(nf)]
    temporal = [TemporalOperator(temporal_type[i], torch.tensor(0.1), t1[i], t2[i], avm=False, beta=2.5, tvar=tvar_temporal[i]) for i in range(nf)]
    logical_c = [LogicalOperator(logical_in, dim=1, avm=avm, beta=2.5, tvar=tvar_logical_c[i]) for i in range(nc)]
    logical_d = LogicalOperator(logical_out, dim=1, avm=avm, beta=2.5, tvar=tvar_logical_d)

    Nmax = Normalization_max(dim=1)
    optimizer = torch.optim.Adam([a, b, t1, t2, pc, pd, tvar_temporal, tvar_logical_c, tvar_logical_d, tau_pos, tau_neg], lr=0.1)
    acc_best = 0

    for epoch in range(1, n_iters):
        rand_idx = rd.sample(range(0, nsample), 64)
        x = train_data[rand_idx, :, :]
        y = train_label[rand_idx]
        x = Nmax.forward(x)

        r1o = torch.empty((x.shape[0], nf), dtype=torch.float64)
        for k, (predi, Ti) in enumerate(zip(pred, temporal)):
            r1 = predi.forward(x)
            r1o[:, k] = Ti.forward(r1, padding=False)

        rc = torch.empty((x.shape[0], nc), dtype=torch.float64)
        for k, li in enumerate(logical_c):
            rc[:, k] = li.forward(r1o, pc[k, :], keepdim=False)

        R = logical_d.forward(rc, pd[0, :], keepdim=False)

        pos_mask = y == 1
        neg_mask = y == -1

        R_pos = R[pos_mask].unsqueeze(1) if pos_mask.sum() > 0 else None
        R_neg = R[neg_mask].unsqueeze(1) if neg_mask.sum() > 0 else None

        coverage_loss = 0.0
        if R_pos is not None and R_neg is not None:
            pos_score = torch.cdist(R_pos, R_neg, p=2)
            neg_score = torch.cdist(R_neg, R_pos, p=2)
            pos_soft_min = soft_min(pos_score, beta=10.0)
            neg_soft_min = soft_min(neg_score, beta=10.0)
            cp_pos = torch.sigmoid((pos_soft_min - tau_pos) / 0.1)
            cp_neg = torch.sigmoid((tau_neg - neg_soft_min) / 0.1)
            coverage_loss += F.relu(0.95 - cp_pos.mean()) + F.relu(0.95 - cp_neg.mean())

        bce_loss = F.binary_cross_entropy_with_logits(R.float(), ((y + 1) / 2).float())

        R_hard = (R > 0).float()
        hard_label = ((y + 1) / 2).float()
        hard_loss = F.binary_cross_entropy(R_hard, hard_label)

        total_loss = bce_loss + 2.0 * coverage_loss + 2.0 * hard_loss
        total_loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if epoch % 100 == 0:
            with torch.no_grad():
                x_val = Nmax.forward(val_data)
                r1o_val = torch.empty((val_nsample, nf), dtype=torch.float64)
                for k, (predi, Ti) in enumerate(zip(pred, temporal)):
                    r1 = predi.forward(x_val)
                    r1o_val[:, k] = Ti.forward(r1, padding=False)
                rc_val = torch.empty((val_nsample, nc), dtype=torch.float64)
                for k, li in enumerate(logical_c):
                    rc_val[:, k] = li.forward(r1o_val, pc[k, :], keepdim=False)
                R_val = logical_d.forward(rc_val, pd[0, :], keepdim=False)
                pred_val = (R_val > 0).float() * 2 - 1
                val_acc = torch.sum(pred_val == val_label) / val_nsample

                print(f"epoch {epoch}, loss = {total_loss.item():.4f}, accuracy = {val_acc.item():.8f}")

                if val_acc > acc_best:
                    acc_best = val_acc.item()
                    with open('W_best.pkl', 'wb') as f:
                        pickle.dump([a, b, t1, t2, pc, pd, tvar_temporal, tvar_logical_c, tvar_logical_d], f)
                    with open('network_best.pkl', 'wb') as f:
                        pickle.dump([pred, temporal, logical_c, logical_d], f)

    return torch.tensor(acc_best), torch.sum(pc)
